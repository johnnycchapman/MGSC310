---
title: "Problem Set 9"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Johnny Chapman
Hersh
MGSC 310

Collabed with: Conrad Frisch

## Load Packages

```{r packages}
library('tidyverse')
library(ggridges)
library(ElemStatLearn)
library('partykit')
library('magrittr')
library(caret)
library(randomForest)
library(randomForestExplainer)
```

### Question A:
Let’s apply the cleaning code to our data to generate data which we can use for modeling.


```{r}
options(scipen = 50)
set.seed(1861)

movies <- read.csv("~/Desktop/Chapman/FALL19/MGSC310/Datasets/movie_metadata.csv")
movies <- movies %>% filter(budget < 4e+08) %>% 
    filter(content_rating != "", 
    content_rating != "Not Rated", plot_keywords != "", !is.na(gross))

movies <- movies %>% mutate(genre_main = unlist(map(strsplit(as.character(movies$genres),
"\\|"), 1)), plot_main = unlist(map(strsplit(as.character(movies$plot_keywords),
"\\|"), 1)), grossM = gross/1e+06, budgetM = budget/1e+06)

movies <- movies %>% mutate(genre_main = fct_lump(genre_main,
7), plot_first = fct_lump(plot_main, 20), content_rating = fct_lump(content_rating,
4), country = fct_lump(country, 8), language = fct_lump(language,
4), cast_total_facebook_likes000s = cast_total_facebook_likes/1000,
) %>% drop_na()
  
top_director <- movies %>% group_by(director_name) %>% summarize(num_films = n()) %>%
top_frac(0.1) %>% mutate(top_director = 1) %>% select(-num_films)
movies <- movies %>% left_join(top_director, by = "director_name") %>%
mutate(top_director = replace_na(top_director, 0)) %>% 

select(-c(director_name,
actor_2_name, gross, genres, actor_1_name, movie_title, actor_3_name,
plot_keywords, movie_imdb_link, budget, color, aspect_ratio,
plot_main, actor_3_facebook_likes, actor_2_facebook_likes,
color, num_critic_for_reviews, num_voted_users, num_user_for_reviews,
actor_2_facebook_likes))
sapply(movies %>% select_if(is.factor), table)


train_idx <- sample(1:nrow(movies), size = floor(0.75 * nrow(movies)))
movies_train <- movies %>% slice(train_idx)
movies_test <- movies %>% slice(-train_idx)
```


### Question B:
Produce a ridgeline plot showing grossM against plot_first. Which plot keywords are associated with the most blockbusters (gross above $300M)?

```{r}
ggplot(movies_train, aes(grossM, plot_first, color = plot_first)) + geom_density_ridges() + labs(x = "Gross (in millions)", y = "Plot First")
```


### Question C:
Fit a bagging model using 100 regression trees predicting grossM using every other variable on the right hand side Let each bootstraped dataset be of size 2000.


```{r}
movie_train_preds <- movies_train %>% rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname))
# bagging - bootstrapp aggregation
B <- 100     # number of bootstrap samples
num_b <- 2000  # sample size of each bootstrap
boot_mods <- list() # store our bagging models
for(i in 1:B){
  boot_idx <- sample(1:nrow(movies_train), 
                     size = num_b,
                     replace = FALSE)

    boot_mod <- ctree(grossM ~ .,  
                     data = movies_train %>% 
                       slice(boot_idx)) 

  
  preds_boot <- data.frame(
    preds_boot = predict(boot_mod),
    rowname = boot_idx 
  )  
  boot_mods[[i]] <- boot_mod
  
  names(preds_boot)[1] <- paste("preds_boot",i,sep = "")
  
  movie_train_preds <- left_join(x = movie_train_preds, y = preds_boot,
                              by = "rowname")
}

```


### Question D:
Summarize across the 100 bagging iterations to generate average predictions for each movie. Call this variable preds_bag.

```{r}
movie_train_preds <- movie_train_preds %>%  
  mutate(preds_bag = select(., preds_boot1:preds_boot100) %>% 
           rowMeans(na.rm = TRUE))

summary(movie_train_preds$preds_bag)
```


### Question E: 
Calculcate R2, RMSE and Mean Absolute Error (using the function MAE in caret) for the bagged predictions. How well does our model do?


```{r}
R2(movie_train_preds$preds_bag, movie_train_preds$grossM)
RMSE(movie_train_preds$preds_bag, movie_train_preds$grossM)
MAE(movie_train_preds$preds_bag, movie_train_preds$grossM)
```


### Question F: 
Fit a random forest model of 500 trees against the movies_train data. Use as mtry the square root of the number of variables – that is the number of columns of the final data used for estimation. (Hint: it’s not sqrt(16)).


```{r}
rf_fit <- randomForest(grossM ~ ., data = movies_train,
                       type = regression,
                       ntree = 100,
                       mtry = 6,
                       importance = TRUE,
                       localIMP = TRUE)

```


### Question G: 
What value did you set mtry to in the problem above? Why is it not sqrt(16)?


```{r}
rf_fit$mtry
```
Because this is a regression problem, we want to use p/3, where p is the number of variables. In this case p is 16, so 16/3 is 5.33 and we round up to 6.



### Question H: 
How does our model improve as we increase the number of trees? Use plot over the function to determine how many trees should be used.


```{r}
plot(rf_fit)
```
The number of trees greatly reduces the amount of error. I would say about 60 trees should be the number of trees used. After 60, the error really does not change all that much and is just making the model computationally more expensive. 

### Question I: 
What are the top 5 most important variables? Use varImpPlot over the random forest object to discover these.


```{r}
varImpPlot(rf_fit)
```


### Question J: 
Use the function plot_min_depth_distribution in the randomForestExplainer package to explore mininum depth by variable. Write 3-5 sentences explaining the plot and its conclusions as you would explain to a client who has no background in machine learning. (Note, this step might take a while. On my laptop this takes about 1-2 minutes. Set ‘catch=TRUE’ as an option on the chunk and every time you knit your document unless you change the code you won’t need to re-run this chunk.)


```{r}
minplot <- plot_min_depth_distribution(rf_fit)

minplot
```


### Question K: 
Use the function plot_predict_interaction() to produce two plots: one exploring the interactions between budgetM and imdb_score and another exploring interactions between budgetM and title_year. Write 3-5 sentences explaining the plots as you would to a nonexpert, including the conclusion we would draw from such a plot.

BudgetM and IMDB_Score
```{r}
grossM_imdbscore <- plot_predict_interaction(rf_fit, movies_train, 
                                     "budgetM", 
                                     "imdb_score")
grossM_imdbscore
```
Once the movie budget is over about $200 million, the imdb_score is predicted to be at least an 8/10 or higher.



BudgetM and Title_Year
```{r}
grossM_title_year <- plot_predict_interaction(rf_fit, movies_train, 
                                             "budgetM", 
                                             "title_year")
grossM_title_year
```
Here there is not much correlation between budget and title year. The years are quite disperse, but the budget seems to somewhat stay above $200 million with the orange squares. There is one prominent prediction at maybe year 2017 and $310 million. 

### Question L: 
Generate test predictions, out of bag predictions (read the help here to learn how to generate them) and in-bag predictions. Compare the performance metrics across the three different types of predictions. Comment on why we see this pattern.


```{r}

preds_test <- predict(rf_fit, newdata = movies_test, type = "response", 
                      norm.votes = TRUE)

preds_outbag <- predict(rf_fit, type = "response", 
                      norm.votes = TRUE)

preds_inbag <- predict(rf_fit, newdata = movies_train, type = "response", 
                       norm.votes = TRUE)
```


Test
```{r}
R2(preds_test, movies_test$grossM)
RMSE(preds_test, movies_test$grossM)
MAE(preds_test, movies_test$grossM)
```


In-Bag
```{r}
R2(preds_inbag, movies_train$grossM)
RMSE(preds_inbag, movies_train$grossM)
MAE(preds_inbag, movies_train$grossM)
```


Out-of-Bag
```{r}
R2(preds_outbag, movies_train$grossM)
RMSE(preds_outbag, movies_train$grossM)
MAE(preds_outbag, movies_train$grossM)
```


We see this pattern because we are using a random forest which means there is a random selection of m predictors chosen as a split candidate. This also means that a fresh set of m predictors is taken at each split. 